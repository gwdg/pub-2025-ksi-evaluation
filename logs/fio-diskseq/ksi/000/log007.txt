+ set -e
+ set -o pipefail
+ '[' -z /nfs/workloads/kind-slurm-integration/benchmark/ksi/workload-fio-diskseq.sh ']'
+ '[' '!' -f /nfs/workloads/kind-slurm-integration/benchmark/ksi/workload-fio-diskseq.sh ']'
++ stat -fc %T /sys/fs/cgroup/
+ '[' cgroup2fs = cgroup2fs ']'
+ echo 'cgroups v2 check passed: cgroups v2 is enabled'
cgroups v2 check passed: cgroups v2 is enabled
++ which podman
++ alias
++ eval declare -f
+++ declare -f
++ /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot podman
+ '[' -x /usr/bin/podman ']'
podman check passed
+ echo 'podman check passed'
++ which kubectl
++ alias
++ eval declare -f
+++ declare -f
++ /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot kubectl
+ '[' -x /usr/local/bin/kubectl ']'
kubectl check passed
+ echo 'kubectl check passed'
++ which kind
++ alias
++ eval declare -f
+++ declare -f
++ /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot kind
+ '[' -x /usr/local/bin/kind ']'
kind check passed
+ echo 'kind check passed'
+ '[' -f /etc/os-release ']'
+ . /etc/os-release
++ NAME='CentOS Stream'
++ VERSION=9
++ ID=centos
++ ID_LIKE='rhel fedora'
++ VERSION_ID=9
++ PLATFORM_ID=platform:el9
++ PRETTY_NAME='CentOS Stream 9'
++ ANSI_COLOR='0;31'
++ LOGO=fedora-logo-icon
++ CPE_NAME=cpe:/o:centos:centos:9
++ HOME_URL=https://centos.org/
++ BUG_REPORT_URL=https://bugzilla.redhat.com/
++ REDHAT_SUPPORT_PRODUCT='Red Hat Enterprise Linux 9'
++ REDHAT_SUPPORT_PRODUCT_VERSION='CentOS Stream'
Successfully read /etc/os-release
+ echo 'Successfully read /etc/os-release'
++ uuidgen
++ tr -d -
++ head -c5
+ cluster_name=20b81
++ random_unused_port
++ local port
++ (( port=30000 ))
++ (( port<=32767 ))
++ ss -Htan
++ awk '{print $4}'
++ cut -d: -f2
++ grep 30000
++ [[ 1 == 1 ]]
++ echo 30000
++ break
+ : 30000
+ export K8S_PORT=30000
+ K8S_PORT=30000
K8S_PORT=30000
+ echo K8S_PORT=30000
+ trap cleanup EXIT
+ trap cleanup SIGTERM
+ trap cleanup SIGINT
Kind config:
+ echo 'Kind config:'
+ envsubst
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraMounts:
  - hostPath: ./
    containerPath: /app
  extraPortMappings:
    - containerPort: 30000
      hostPort: 30000
      # optional: set the bind address on the host
      # 0.0.0.0 is the current default
      listenAddress: "0.0.0.0"
      # optional: set the protocol to one of TCP, UDP, SCTP.
      # TCP is the default
      protocol: TCP
+ [[ CentOS Stream == \C\e\n\t\O\S\ \S\t\r\e\a\m ]]
+ [[ 9 = \8 ]]
+ envsubst
+ KIND_EXPERIMENTAL_PROVIDER=podman
+ kind create cluster --name 20b81 --wait 5m --config -
using podman due to KIND_EXPERIMENTAL_PROVIDER
enabling experimental podman provider
Creating cluster "20b81" ...
 • Ensuring node image (kindest/node:v1.27.3) 🖼  ...
 ✓ Ensuring node image (kindest/node:v1.27.3) 🖼
 • Preparing nodes 📦   ...
 ✓ Preparing nodes 📦 
 • Writing configuration 📜  ...
 ✓ Writing configuration 📜
 • Starting control-plane 🕹️  ...
 ✓ Starting control-plane 🕹️
 • Installing CNI 🔌  ...
 ✓ Installing CNI 🔌
 • Installing StorageClass 💾  ...
 ✓ Installing StorageClass 💾
 • Waiting ≤ 5m0s for control-plane = Ready ⏳  ...
 ✓ Waiting ≤ 5m0s for control-plane = Ready ⏳
 • Ready after 15s 💚
Set kubectl context to "kind-20b81"
You can now use your cluster with:

kubectl cluster-info --context kind-20b81

Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/
+ kubectl get nodes --context kind-20b81
NAME                  STATUS   ROLES           AGE   VERSION
20b81-control-plane   Ready    control-plane   21s   v1.27.3
+ kubectl cluster-info --context kind-20b81
[0;32mKubernetes control plane[0m is running at [0;33mhttps://127.0.0.1:41617[0m
[0;32mCoreDNS[0m is running at [0;33mhttps://127.0.0.1:41617/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy[0m

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
+ K8S_CLUSTER_NAME=kind-20b81
+ export K8S_CLUSTER_NAME=kind-20b81
+ K8S_CLUSTER_NAME=kind-20b81
++ kubectl config view -o 'jsonpath={.clusters[?(@.name=='\''kind-20b81'\'')].cluster.server}'
+ K8S_CLUSTER_API=https://127.0.0.1:41617
+ export K8S_CLUSTER_API=https://127.0.0.1:41617
+ K8S_CLUSTER_API=https://127.0.0.1:41617
+ kubectl --context kind-20b81 create -f -
serviceaccount/admin-user created
+ kubectl --context kind-20b81 create -f -
clusterrolebinding.rbac.authorization.k8s.io/admin-user created
++ kubectl --context kind-20b81 -n kube-system create token admin-user
+ K8S_CLUSTER_API_TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6IklIQW05WDlrbWhOZjU4S3RmcWlPaHRrNE1DQjlsZUMzaEZxTFlSTmNncjQifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzAwMzE4Mzc3LCJpYXQiOjE3MDAzMTQ3NzcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiOTBjZGI2NmEtNzcwZS00ODUzLTk3YWMtYjdiNGIwOTBhMGU3In19LCJuYmYiOjE3MDAzMTQ3NzcsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbi11c2VyIn0.FKhcfva-s_awQhUxHPKKSH92kcIrOee60S5oR19ocPsggZeum8rAzRouJY1-ij7l_GAeljCE7Axzp7wLlBay0-Cocrzm12yn08_UB_MbSZ-9RfJQxJa34A9D9Y3RI3ORmU0qkzWXHKfFO1ziP6Psj6sKiY7uIIbn4rYoOBaPz1u4oXH0PGZEaSrdiKC1Zdh2ZZc3ZBkKXlEsp06L6HbnEGZ3d3lrUJjdIu4KU_gNQKSajrflqov0RZiagKxD-yzgosRgG2fuH8erucGgroBq7FXVKi5mLoQyFJ6QO4NOmztDH9eUAeGDK8m0KP4ckwq_BD1rlV9-GrRv9KTFWeP1Og
+ export K8S_CLUSTER_API_TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6IklIQW05WDlrbWhOZjU4S3RmcWlPaHRrNE1DQjlsZUMzaEZxTFlSTmNncjQifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzAwMzE4Mzc3LCJpYXQiOjE3MDAzMTQ3NzcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiOTBjZGI2NmEtNzcwZS00ODUzLTk3YWMtYjdiNGIwOTBhMGU3In19LCJuYmYiOjE3MDAzMTQ3NzcsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbi11c2VyIn0.FKhcfva-s_awQhUxHPKKSH92kcIrOee60S5oR19ocPsggZeum8rAzRouJY1-ij7l_GAeljCE7Axzp7wLlBay0-Cocrzm12yn08_UB_MbSZ-9RfJQxJa34A9D9Y3RI3ORmU0qkzWXHKfFO1ziP6Psj6sKiY7uIIbn4rYoOBaPz1u4oXH0PGZEaSrdiKC1Zdh2ZZc3ZBkKXlEsp06L6HbnEGZ3d3lrUJjdIu4KU_gNQKSajrflqov0RZiagKxD-yzgosRgG2fuH8erucGgroBq7FXVKi5mLoQyFJ6QO4NOmztDH9eUAeGDK8m0KP4ckwq_BD1rlV9-GrRv9KTFWeP1Og
+ K8S_CLUSTER_API_TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6IklIQW05WDlrbWhOZjU4S3RmcWlPaHRrNE1DQjlsZUMzaEZxTFlSTmNncjQifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzAwMzE4Mzc3LCJpYXQiOjE3MDAzMTQ3NzcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiOTBjZGI2NmEtNzcwZS00ODUzLTk3YWMtYjdiNGIwOTBhMGU3In19LCJuYmYiOjE3MDAzMTQ3NzcsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbi11c2VyIn0.FKhcfva-s_awQhUxHPKKSH92kcIrOee60S5oR19ocPsggZeum8rAzRouJY1-ij7l_GAeljCE7Axzp7wLlBay0-Cocrzm12yn08_UB_MbSZ-9RfJQxJa34A9D9Y3RI3ORmU0qkzWXHKfFO1ziP6Psj6sKiY7uIIbn4rYoOBaPz1u4oXH0PGZEaSrdiKC1Zdh2ZZc3ZBkKXlEsp06L6HbnEGZ3d3lrUJjdIu4KU_gNQKSajrflqov0RZiagKxD-yzgosRgG2fuH8erucGgroBq7FXVKi5mLoQyFJ6QO4NOmztDH9eUAeGDK8m0KP4ckwq_BD1rlV9-GrRv9KTFWeP1Og
Executing the Kubernetes workload script /nfs/workloads/kind-slurm-integration/benchmark/ksi/workload-fio-diskseq.sh on cluster kind-20b81
+ echo 'Executing the Kubernetes workload script /nfs/workloads/kind-slurm-integration/benchmark/ksi/workload-fio-diskseq.sh on cluster kind-20b81'
+ wait
+ /bin/bash /nfs/workloads/kind-slurm-integration/benchmark/ksi/workload-fio-diskseq.sh
+ kubectl create --context kind-20b81 namespace bench
namespace/bench created
+ kubectl create -n bench --context kind-20b81 -f -
job.batch/fio created
+ kubectl wait --context kind-20b81 --for=condition=complete --timeout=10h job/fio -n bench
job.batch/fio condition met
+ kubectl logs job/fio -n bench --context kind-20b81
kube-fio-write: (g=0): rw=write, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=psync, iodepth=1
fio-3.35
Starting 1 process
3;fio-3.35;kube-fio-write;0;0;0;0;0;0;0;0;0.000000;0.000000;0;0;0.000000;0.000000;1.000000%=0;5.000000%=0;10.000000%=0;20.000000%=0;30.000000%=0;40.000000%=0;50.000000%=0;60.000000%=0;70.000000%=0;80.000000%=0;90.000000%=0;95.000000%=0;99.000000%=0;99.500000%=0;99.900000%=0;99.950000%=0;99.990000%=0;0%=0;0%=0;0%=0;0;0;0.000000;0.000000;0;0;0.000000%;0.000000;0.000000;8388608;74247;290;112982;0;0;0.000000;0.000000;83;319811;3441.940038;8270.457983;1.000000%=95;5.000000%=97;10.000000%=98;20.000000%=100;30.000000%=107;40.000000%=112;50.000000%=118;60.000000%=152;70.000000%=189;80.000000%=7176;90.000000%=12517;95.000000%=17694;99.000000%=31064;99.500000%=36438;99.900000%=64225;99.950000%=102236;99.990000%=258998;0%=0;0%=0;0%=0;87;319819;3446.748931;8270.537034;1024;1015808;100.000000%;74346.951111;98593.105726;0.189412%;4.202477%;9650;2;152;100.0%;0.0%;0.0%;0.0%;0.0%;0.0%;0.0%;0.00%;0.00%;0.00%;0.00%;0.00%;16.99%;57.91%;0.63%;0.02%;0.00%;0.34%;0.10%;11.31%;9.59%;2.93%;0.12%;0.04%;0.01%;0.00%;0.00%;0.00%;0.00%;dm-0;1668;3252;0;0;204454;145517;349971;57.58%;slaves;36390;33998;5039;450;3751057;346984;4120821;92.29%;sda;36390;33998;5039;450;3751057;346984;4120821;92.29%

kube-fio-write: (groupid=0, jobs=1): err= 0: pid=72: Sat Nov 18 13:41:41 2023
  write: IOPS=290, BW=72.5MiB/s (76.0MB/s)(8192MiB/112982msec); 0 zone resets
    clat (usec): min=83, max=319811, avg=3441.94, stdev=8270.46
     lat (usec): min=87, max=319819, avg=3446.75, stdev=8270.54
    clat percentiles (usec):
     |  1.00th=[    96],  5.00th=[    98], 10.00th=[    99], 20.00th=[   101],
     | 30.00th=[   108], 40.00th=[   113], 50.00th=[   119], 60.00th=[   153],
     | 70.00th=[   190], 80.00th=[  7177], 90.00th=[ 12518], 95.00th=[ 17695],
     | 99.00th=[ 31065], 99.50th=[ 36439], 99.90th=[ 64226], 99.95th=[102237],
     | 99.99th=[258999]
   bw (  KiB/s): min= 1024, max=1015808, per=100.00%, avg=74346.95, stdev=98593.11, samples=225
   iops        : min=    4, max= 3968, avg=290.42, stdev=385.13, samples=225
  lat (usec)   : 100=16.99%, 250=57.91%, 500=0.63%, 750=0.02%
  lat (msec)   : 2=0.34%, 4=0.10%, 10=11.31%, 20=9.59%, 50=2.93%
  lat (msec)   : 100=0.12%, 250=0.04%, 500=0.01%
  cpu          : usr=0.19%, sys=4.20%, ctx=9650, majf=2, minf=152
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,32768,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
  WRITE: bw=72.5MiB/s (76.0MB/s), 72.5MiB/s-72.5MiB/s (76.0MB/s-76.0MB/s), io=8192MiB (8590MB), run=112982-112982msec

Disk stats (read/write):
    dm-0: ios=1668/3252, merge=0/0, ticks=204454/145517, in_queue=349971, util=57.58%, aggrios=36390/33998, aggrmerge=5039/450, aggrticks=3751057/346984, aggrin_queue=4120821, aggrutil=92.29%
  sda: ios=36390/33998, merge=5039/450, ticks=3751057/346984, in_queue=4120821, util=92.29%
kube-fio-read: (g=0): rw=read, bs=(R) 256KiB-256KiB, (W) 256KiB-256KiB, (T) 256KiB-256KiB, ioengine=psync, iodepth=1
fio-3.35
Starting 1 process
3;fio-3.35;kube-fio-read;0;0;8388608;288397;1126;29087;0;0;0.000000;0.000000;23;413598;887.061771;6372.252227;1.000000%=25;5.000000%=28;10.000000%=30;20.000000%=34;30.000000%=36;40.000000%=37;50.000000%=38;60.000000%=41;70.000000%=48;80.000000%=58;90.000000%=69;95.000000%=3719;99.000000%=18219;99.500000%=29229;99.900000%=84410;99.950000%=122159;99.990000%=223346;0%=0;0%=0;0%=0;23;413598;887.129717;6372.249573;32768;451584;99.896013%;288097.103448;124704.534818;0;0;0;0;0;0;0.000000;0.000000;0;0;0.000000;0.000000;1.000000%=0;5.000000%=0;10.000000%=0;20.000000%=0;30.000000%=0;40.000000%=0;50.000000%=0;60.000000%=0;70.000000%=0;80.000000%=0;90.000000%=0;95.000000%=0;99.000000%=0;99.500000%=0;99.900000%=0;99.950000%=0;99.990000%=0;0%=0;0%=0;0%=0;0;0;0.000000;0.000000;0;0;0.000000%;0.000000;0.000000;0.175336%;8.584591%;4882;1;152;100.0%;0.0%;0.0%;0.0%;0.0%;0.0%;0.0%;0.00%;0.00%;0.00%;0.00%;70.58%;21.26%;0.04%;0.02%;0.02%;0.03%;1.77%;2.43%;1.84%;1.11%;0.65%;0.19%;0.06%;0.01%;0.00%;0.00%;0.00%;0.00%;dm-0;3161;161;0;0;154765;30228;184993;89.97%;slaves;59923;1998;3273;20;3310734;60770;3374518;96.90%;sda;59923;1998;3273;20;3310734;60770;3374518;96.90%

kube-fio-read: (groupid=0, jobs=1): err= 0: pid=130: Sat Nov 18 13:42:11 2023
  read: IOPS=1126, BW=282MiB/s (295MB/s)(8192MiB/29087msec)
    clat (usec): min=23, max=413598, avg=887.06, stdev=6372.25
     lat (usec): min=23, max=413598, avg=887.13, stdev=6372.25
    clat percentiles (usec):
     |  1.00th=[    26],  5.00th=[    29], 10.00th=[    31], 20.00th=[    35],
     | 30.00th=[    37], 40.00th=[    38], 50.00th=[    39], 60.00th=[    42],
     | 70.00th=[    49], 80.00th=[    59], 90.00th=[    70], 95.00th=[  3720],
     | 99.00th=[ 18220], 99.50th=[ 29230], 99.90th=[ 84411], 99.95th=[122160],
     | 99.99th=[223347]
   bw (  KiB/s): min=32768, max=451584, per=99.90%, avg=288097.10, stdev=124704.53, samples=58
   iops        : min=  128, max= 1764, avg=1125.38, stdev=487.13, samples=58
  lat (usec)   : 50=70.58%, 100=21.26%, 250=0.04%, 500=0.02%, 750=0.02%
  lat (usec)   : 1000=0.03%
  lat (msec)   : 2=1.77%, 4=2.43%, 10=1.84%, 20=1.11%, 50=0.65%
  lat (msec)   : 100=0.19%, 250=0.06%, 500=0.01%
  cpu          : usr=0.18%, sys=8.58%, ctx=4882, majf=1, minf=152
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=32768,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=1

Run status group 0 (all jobs):
   READ: bw=282MiB/s (295MB/s), 282MiB/s-282MiB/s (295MB/s-295MB/s), io=8192MiB (8590MB), run=29087-29087msec

Disk stats (read/write):
    dm-0: ios=3161/161, merge=0/0, ticks=154765/30228, in_queue=184993, util=89.97%, aggrios=59923/1998, aggrmerge=3273/20, aggrticks=3310734/60770, aggrin_queue=3374518, aggrutil=96.90%
  sda: ios=59923/1998, merge=3273/20, ticks=3310734/60770, in_queue=3374518, util=96.90%
+ kubectl delete --context kind-20b81 namespace bench
namespace "bench" deleted
+ cleanup
Deleting Kind cluster container 20b81
+ echo 'Deleting Kind cluster container 20b81'
+ [[ CentOS Stream == \C\e\n\t\O\S\ \S\t\r\e\a\m ]]
+ [[ 9 = \8 ]]
+ KIND_EXPERIMENTAL_PROVIDER=podman
+ kind delete cluster --name 20b81
using podman due to KIND_EXPERIMENTAL_PROVIDER
enabling experimental podman provider
Deleting cluster "20b81" ...
Deleted nodes: ["20b81-control-plane"]
